{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Handling Numerical Data"
      ],
      "metadata": {
        "id": "HQ75jx_59PDb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Analysis of the Dataset"
      ],
      "metadata": {
        "id": "DK61EkQdr_sX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axSayC7DqmQN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the csv file into a pandas DataFrame\n",
        "df = pd.read_csv('Prakriti_With_Features.csv')\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display information about the DataFrame, including data types and non-null values\n",
        "print(\"\\nDataFrame Info:\")\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Numerical Data"
      ],
      "metadata": {
        "id": "E9oBhugyr6Qu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create a copy to avoid modifying the original DataFrame\n",
        "df_scaled = df.copy()\n",
        "\n",
        "# Define a mapping for the 'Height' column\n",
        "height_mapping = {'Short': 0, 'Average': 1, 'Tall': 2}\n",
        "df_scaled['Height_numeric'] = df_scaled['Height'].map(height_mapping)\n",
        "\n",
        "# Reshape the data for the scaler\n",
        "height_data = df_scaled['Height_numeric'].values.reshape(-1, 1)\n",
        "\n",
        "# Create a scaler and fit it to the data\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled['Height_scaled'] = scaler.fit_transform(height_data)\n",
        "\n",
        "print(\"Original and scaled 'Height' data:\")\n",
        "print(df_scaled[['Height', 'Height_numeric', 'Height_scaled']].head())"
      ],
      "metadata": {
        "id": "o77noc9Oq42b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standardizing a Feature"
      ],
      "metadata": {
        "id": "yKPqaJixr0xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a copy for this operation\n",
        "df_standardized = df.copy()\n",
        "\n",
        "# Use the same height mapping\n",
        "df_standardized['Height_numeric'] = df_standardized['Height'].map(height_mapping)\n",
        "height_data = df_standardized['Height_numeric'].values.reshape(-1, 1)\n",
        "\n",
        "# Create a StandardScaler and apply it\n",
        "standard_scaler = StandardScaler()\n",
        "df_standardized['Height_standardized'] = standard_scaler.fit_transform(height_data)\n",
        "\n",
        "print(\"Original, numeric, and standardized 'Height' data:\")\n",
        "print(df_standardized[['Height', 'Height_numeric', 'Height_standardized']].head())"
      ],
      "metadata": {
        "id": "G_LrOLOdq8mX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing Observations"
      ],
      "metadata": {
        "id": "hSczfWAwruuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "# Create a copy for this operation\n",
        "df_normalized = df.copy()\n",
        "\n",
        "# Define mappings\n",
        "height_mapping = {'Short': 0, 'Average': 1, 'Tall': 2}\n",
        "body_size_mapping = {'Slim': 0, 'Medium': 1, 'Large': 2}\n",
        "\n",
        "df_normalized['Height_numeric'] = df_normalized['Height'].map(height_mapping)\n",
        "df_normalized['Body_Size_numeric'] = df_normalized['Body Size'].map(body_size_mapping)\n",
        "\n",
        "# Select the numerical features\n",
        "features = df_normalized[['Height_numeric', 'Body_Size_numeric']].values\n",
        "\n",
        "# Create a Normalizer and apply it\n",
        "normalizer = Normalizer(norm='l2')\n",
        "df_normalized[['Height_normalized', 'Body_Size_normalized']] = normalizer.transform(features)\n",
        "\n",
        "print(\"Original, numeric, and normalized 'Height' and 'Body Size' data:\")\n",
        "print(df_normalized[['Height', 'Body Size', 'Height_numeric', 'Body_Size_numeric', 'Height_normalized', 'Body_Size_normalized']].head())"
      ],
      "metadata": {
        "id": "-AA3D9CZrAec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouping Observations Using Clustering"
      ],
      "metadata": {
        "id": "egTi3-urrnWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a copy for this operation\n",
        "df_clustered = df.copy()\n",
        "\n",
        "# Use the same mappings as before\n",
        "height_mapping = {'Short': 0, 'Average': 1, 'Tall': 2}\n",
        "body_size_mapping = {'Slim': 0, 'Medium': 1, 'Large': 2}\n",
        "\n",
        "df_clustered['Height_numeric'] = df_clustered['Height'].map(height_mapping)\n",
        "df_clustered['Body_Size_numeric'] = df_clustered['Body Size'].map(body_size_mapping)\n",
        "\n",
        "# Select the features for clustering\n",
        "features_for_clustering = df_clustered[['Height_numeric', 'Body_Size_numeric']]\n",
        "\n",
        "# Create a KMeans model with 3 clusters\n",
        "kmeans = KMeans(n_clusters=3, random_state=0, n_init=10)\n",
        "\n",
        "# Fit the model and predict the clusters\n",
        "df_clustered['group'] = kmeans.fit_predict(features_for_clustering)\n",
        "\n",
        "print(\"Data with assigned cluster groups:\")\n",
        "print(df_clustered[['Height', 'Body Size', 'group']].head())"
      ],
      "metadata": {
        "id": "pRSiPqwhrFaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deleting Observations with Missing Values"
      ],
      "metadata": {
        "id": "gzBj8zJerhGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create a copy with some missing values\n",
        "df_missing = df.copy()\n",
        "df_missing.loc[0, 'Body Size'] = np.nan\n",
        "df_missing.loc[2, 'Height'] = np.nan\n",
        "\n",
        "print(\"DataFrame with missing values:\")\n",
        "print(df_missing.head())\n",
        "\n",
        "# Drop rows with any missing values\n",
        "df_dropped = df_missing.dropna()\n",
        "\n",
        "print(\"\\nDataFrame after dropping rows with missing values:\")\n",
        "print(df_dropped.head())"
      ],
      "metadata": {
        "id": "rHQasHifrJHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputing Missing Values"
      ],
      "metadata": {
        "id": "siDYba9yrXwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Create a copy with missing values\n",
        "df_to_impute = df.copy()\n",
        "df_to_impute.loc[0, 'Body Size'] = np.nan\n",
        "df_to_impute.loc[2, 'Height'] = np.nan\n",
        "\n",
        "print(\"DataFrame with missing values:\")\n",
        "print(df_to_impute.head())\n",
        "\n",
        "# Create an imputer that fills missing values with the most frequent value\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Apply the imputer to the DataFrame\n",
        "df_imputed_array = imputer.fit_transform(df_to_impute)\n",
        "df_imputed = pd.DataFrame(df_imputed_array, columns=df_to_impute.columns)\n",
        "\n",
        "print(\"\\nDataFrame after imputing missing values:\")\n",
        "print(df_imputed.head())"
      ],
      "metadata": {
        "id": "QywwDPSjrN07"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
