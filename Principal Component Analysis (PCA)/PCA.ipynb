{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing: Converting Categorical Data to Numerical"
      ],
      "metadata": {
        "id": "XUmJIBiFQ6Sy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwmHiTTeIduu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Prakriti_With_Features.csv')\n",
        "\n",
        "# Separate features (X) and the target variable (y)\n",
        "X = df.drop('Dosha', axis=1)\n",
        "y_categorical = df['Dosha']\n",
        "\n",
        "# --- Preprocess Features (X) ---\n",
        "# One-hot encode the categorical features\n",
        "X_numerical = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "# Standardize the feature matrix (important for PCA and LDA)\n",
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X_numerical)\n",
        "\n",
        "# --- Preprocess Target (y) for LDA ---\n",
        "# Encode the categorical target variable into numbers\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y_categorical)\n",
        "\n",
        "print(f\"Original number of features after one-hot encoding: {X_std.shape[1]}\")\n",
        "print(f\"Number of samples: {X_std.shape[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Reducing Features Using Principal Components (PCA)"
      ],
      "metadata": {
        "id": "bB90vVtHQa0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Create a PCA instance to retain 99% of the variance\n",
        "# whiten=True can help some algorithms by scaling components to have unit variance\n",
        "pca = PCA(n_components=0.99, whiten=True)\n",
        "\n",
        "# Conduct PCA on the standardized data\n",
        "X_pca = pca.fit_transform(X_std)\n",
        "\n",
        "# Show the results\n",
        "print('Original number of features:', X_std.shape[1])\n",
        "print('Reduced number of features:', X_pca.shape[1])"
      ],
      "metadata": {
        "id": "Fd6edjMbQbub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Reducing Features When Data Is Linearly Inseparable (Kernel PCA)"
      ],
      "metadata": {
        "id": "CrHOHgkKRFEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Apply Kernel PCA with an RBF kernel\n",
        "kpca = KernelPCA(kernel=\"rbf\", gamma=15, n_components=1)\n",
        "X_kpca = kpca.fit_transform(X_std)\n",
        "\n",
        "print('Original number of features:', X_std.shape[1])\n",
        "print('Reduced number of features:', X_kpca.shape[1])"
      ],
      "metadata": {
        "id": "QSgtO6FIQqo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Reducing Features by Maximizing Class Separability (LDA)"
      ],
      "metadata": {
        "id": "4J7SIjZURJnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "# Create an LDA that will reduce the data down to 1 feature\n",
        "# Note: The number of components in LDA is at most n_classes - 1\n",
        "lda = LinearDiscriminantAnalysis(n_components=1)\n",
        "\n",
        "# Run LDA using both features (X_std) and the target (y)\n",
        "X_lda = lda.fit(X_std, y).transform(X_std)\n",
        "\n",
        "# Print the number of features\n",
        "print('Original number of features:', X_std.shape[1])\n",
        "print('Reduced number of features:', X_lda.shape[1])\n",
        "\n",
        "# View the amount of variance explained by the component\n",
        "print('\\nVariance explained by the new feature:', lda.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "2fErEV_GQwNq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}